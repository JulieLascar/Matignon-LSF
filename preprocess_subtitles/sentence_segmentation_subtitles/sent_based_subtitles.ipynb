{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing subtitles\n",
    "\n",
    "This notebook is used to get one sentence per subtitles in the .vtt files from the **Matignon_LSF** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import module_traitement as m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the folders\n",
    "\n",
    "Here, we load the folder containing the subtitles (.vtt files).\n",
    "1. file_with_path : list of `.vtt` files with their relative path\n",
    "2. folder : list of  `.vtt` file with only their file's names\n",
    "\n",
    "We create **two output** folders : \n",
    "1. One for the cleaned subtitles (`.vtt`) files \n",
    "1. One for the segmented files based on punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST -  Matignon-LSF with one file  \n",
    "file_with_path = m.lister_fichiers_with_path(\"test_matignon/\")\n",
    "folder = m.lister_fichiers(\"test_matignon/\")\n",
    "output_seg = \"test_new_seg_matignon1\"\n",
    "output_cleaning = \"test_cleaning_matignon1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the folder exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le dossier 'test_cleaning_matignon1' existe déjà.\n",
      "Le dossier 'test_new_seg_matignon1' existe déjà.\n"
     ]
    }
   ],
   "source": [
    "for fold in [output_cleaning,output_seg]:\n",
    "    message = m.verifier_ou_creer_dossier(fold)\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files normalisation\n",
    "\n",
    "First, we use regexp to clean the subtitles files. It makes it easier to have the \"sentence per subtitles\" files at the end. We manage all the punctuations which could cause trouble for the segmentation. The files are stored in the **output_cleaning** folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_grand_nombre(nombre_texte):\n",
    "    nombre_sans_points = re.sub(r'(\\d)\\.(\\d)', r'\\1\\2', nombre_texte)\n",
    "    return nombre_sans_points\n",
    "\n",
    "def remplacer_points_adresses_email(texte):\n",
    "    # regexp for email\n",
    "    pattern = r'\\b[\\w.%+-]+@[\\w.-]+\\.[a-zA-Z]{2,}\\b'\n",
    "\n",
    "    # replace \".\" with \"POINT\"\n",
    "    def remplacer(match):\n",
    "        return match.group().replace('.', 'POINT')\n",
    "\n",
    "    texte_modifie = re.sub(pattern, remplacer, texte)\n",
    "    return texte_modifie\n",
    "\n",
    "def remplacer_points_adresses(texte):\n",
    "    # regexp for website\n",
    "    pattern = r'\\b(?:https?://)?(?:www\\.)?[\\w.-]+\\.[a-zA-Z]{2,}\\b'\n",
    "\n",
    "    # Fonction de remplacement pour remplacer les points par \"POINT\"\n",
    "    def remplacer(match):\n",
    "        return match.group().replace('.', 'POINT')\n",
    "\n",
    "    texte_modifie = re.sub(pattern, remplacer, texte)\n",
    "    return texte_modifie\n",
    "\n",
    "def normaliser_points_de_suspension(texte):\n",
    "    #add a space after \"...\"\n",
    "    texte_modifie = re.sub(r'\\.\\.\\.(\\w)', r'... \\1', texte)\n",
    "\n",
    "    # Remove a space before ellipses preceded by a letter.\n",
    "    texte_modifie = re.sub(r'(\\w)\\s*\\.\\.\\.', r'\\1...', texte_modifie)\n",
    "\n",
    "    return texte_modifie\n",
    "\n",
    "def remplacer_ponctuation_html(texte):\n",
    "    substitutions = [\n",
    "        (\"(\", \"&#40;\"),\n",
    "        (\")\", \"&#41;\"),\n",
    "        (\"?\", \"&#63;\"),\n",
    "        (\"!\", \"&#33;\"),\n",
    "        (\".\", \"&#46;\"),\n",
    "        (\"...\", \"&#8230;\")\n",
    "    ]\n",
    "\n",
    "    def remplacer_match(match):\n",
    "        contenu_parentheses = match.group(1)\n",
    "        for recherche, remplacement in substitutions:\n",
    "            contenu_parentheses = contenu_parentheses.replace(recherche, remplacement)\n",
    "        return f'({contenu_parentheses})'\n",
    "\n",
    "    texte_modifie = re.sub(r'\\(([^)]*)\\)', remplacer_match, texte)\n",
    "    return texte_modifie\n",
    "\n",
    "def get_dict_vtt_clean(input):\n",
    "    with open(input,encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    dict_sub = {}\n",
    "    i = 0\n",
    "    j = 0  \n",
    "\n",
    "    while j < len(lines): \n",
    "        element = lines[j]\n",
    "        if element.startswith(\"00:\") or element.startswith(\"01:\") or element.startswith(\"02:\"):\n",
    "            # Extract start and end time\n",
    "            timing_line = element.strip().split(' --> ')\n",
    "            start_time, end_time = timing_line\n",
    "\n",
    "            text = \"\"\n",
    "            while j + 1 < len(lines) and not lines[j + 1].startswith(\"00:\") and not lines[j+1].startswith(\"01:\") and not lines[j+1].startswith(\"02:\"):\n",
    "                j += 1\n",
    "                content = lines[j]\n",
    "                text = text + \" \" + content.strip()\n",
    "                text = normaliser_points_de_suspension(text)\n",
    "                text = text.replace(\"(...)\",\"[SUSPENSIONP]\") #régler ce problème.\n",
    "                text = text.replace(\"....\",\"[POINTS4]\")\n",
    "                text = remplacer_ponctuation_html(text)\n",
    "                ### changement\n",
    "                text = text.replace(\"…\",\"...\")\n",
    "                text = text.replace(\"...\",\"[SUSPENSION].\") ### gérer la double suspension !\n",
    "                text = re.sub(r'[\"“”«»]', '', text)\n",
    "                text = text.replace(\"(???)\",\"[INTERROGATION3]\")\n",
    "                text = text.replace(\"?!\",\"[INTEREXCL].\")\n",
    "                text = text.replace(\"(?)\",\"[INTERROGATION1]\")\n",
    "                text = text.replace(\"(??)\",\"[INTERROGATION2]\")\n",
    "                text = text.replace(\"( ?)\",\"[INTERROGATION1]\")\n",
    "                text = text.replace(\"?,\",\"[INTERROGATION],\")\n",
    "                text = text.replace(\"!,\",\"[EXCLAMATION],\")\n",
    "                text = text.replace(\"!.\",\"[EXCLAMPOINT],\")\n",
    "                text = text.replace(\"etc.,\",\"[ETC],\")\n",
    "                text = text.replace(\"etc.)\",\"[ETC])\")\n",
    "                text = text.replace(\"etc. .\",\"[ETC].\")\n",
    "                text = text.replace(\"Etc\",\"etc\")\n",
    "                text = text.replace(\"PAM !\",\"[PAM]\")\n",
    "                text = text.replace(\"Média’Pi\",\"Média'Pi\")\n",
    "                text = text.replace(\"Média'Pi!\",\"[NOM_MEDIA]\")\n",
    "                text = text.replace(\"Média'Pi !\",\"[NOM_MEDIA]\")\n",
    "                text = text.replace(\"Media'Pi !\",\"[NOM_MEDIA]\")\n",
    "                text = text.replace(\"Média'Pi&nbsp;!\",\"[NOM_MEDIA]\")\n",
    "                text = text.replace(\"Média' Pi !\",\"[NOM_MEDIA]\")\n",
    "                \n",
    "                text = text.replace(\".e.s\",\"\")\n",
    "                text = text.replace(\".ne.s\",\"\")\n",
    "                text = text.replace(\".e.\",\"\")\n",
    "                text = text.replace(\".e\",\"\")\n",
    "                text = text.replace(\"!!\",\"!\")\n",
    "                text = text.replace(\"??\",\"?\")\n",
    "                text = remplacer_points_adresses_email(text)\n",
    "                text = remplacer_points_adresses(text)\n",
    "                text = text.replace(\".,\",\"[POINT],\")\n",
    "                text = text.replace(\"y.a\",\"y a\")\n",
    "                ### \n",
    "                text=text.replace(\"... -G. Attal : \",\"\")\n",
    "                text=text.replace(\"-G. Attal : \",\"\")\n",
    "                text=text.replace(\"G. Attal : \",\"\")\n",
    "                text = text.replace(\"-Bonjour\", \"Bonjour\")\n",
    "                text = text.replace(\"Bonjour.\", \"Bonjour,\")\n",
    "                text = text.replace(\" M.\",\" Monsieur\")\n",
    "                text = text.replace(\" Mme\",\" Madame\")\n",
    "                text = text.replace(\"-\",\" \")\n",
    "                text = convertir_grand_nombre(text)\n",
    "                \n",
    "\n",
    "            dict_sub[i] = {'start': start_time, 'end': end_time, 'text': text.strip()}\n",
    "            i += 1\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    return dict_sub\n",
    "\n",
    "\n",
    "\n",
    "def remplacer_entites_html(chaine):\n",
    "    substitutions = [\n",
    "    (\"(\", \"&#40;\"),\n",
    "    (\")\", \"&#41;\"),\n",
    "    (\"?\", \"&#63;\"),\n",
    "    (\"!\", \"&#33;\"),\n",
    "    (\".\", \"&#46;\"),\n",
    "    (\"...\", \"&#8230;\")\n",
    "]\n",
    "    for new, old in substitutions:\n",
    "        chaine = chaine.replace(old, new)\n",
    "    return chaine\n",
    "\n",
    "def reverse_cleaning(input):\n",
    "    with open(input,encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    dict_sub = {}\n",
    "    i = 0\n",
    "    j = 0  \n",
    "\n",
    "    while j < len(lines): \n",
    "        element = lines[j]\n",
    "        if element.startswith(\"00:\") or element.startswith(\"01:\") or element.startswith(\"02:\"):\n",
    "            #Extract end and start time \n",
    "            timing_line = element.strip().split(' --> ')\n",
    "            start_time, end_time = timing_line\n",
    "\n",
    "            text = \"\"\n",
    "            while j + 1 < len(lines) and not lines[j + 1].startswith(\"00:\") and not lines[j+1].startswith(\"01:\") and not lines[j+1].startswith(\"02:\"):\n",
    "                j += 1\n",
    "                content = lines[j]\n",
    "                text = text + \" \" + content.strip()\n",
    "                text = text.replace(\"[POINT],\",\".,\")\n",
    "                text = text.replace(\"[SUSPENSIONP]\",\"(...)\") \n",
    "                text = text.replace(\"[POINTS4]\",\"....\")\n",
    "                text = text.replace(\"…\",\"...\")\n",
    "                text = text.replace(\"[INTERROGATION3]\",\"(???)\")\n",
    "                text = text.replace(\"[INTEREXCL].\",\"?!\")\n",
    "                text = text.replace(\"[INTERROGATION1]\",\"(?)\")\n",
    "                text = text.replace(\"[INTERROGATION2]\",\"(??)\")\n",
    "                text = text.replace(\"[INTERROGATION],\",\"?,\")\n",
    "                text = text.replace(\"[EXCLAMATION],\",\"!,\")\n",
    "                text = text.replace(\"[EXCLAMPOINT],\",\"!.\")\n",
    "                text = text.replace(\"[ETC],\",\"etc.,\")\n",
    "                text = text.replace(\"[ETC])\",\"etc.)\")\n",
    "                text = text.replace(\"[ETC].\",\"etc.\")\n",
    "                text = text.replace(\"[PAM]\",\"PAM !\")\n",
    "                text = text.replace(\"[NOM_MEDIA]\",\"Média'Pi !\")\n",
    "                text = text.replace(\"[SUSPENSION].\",\"...\") \n",
    "                text = text.replace(\"POINT\",\".\")\n",
    "                text = remplacer_entites_html(text)\n",
    "                \n",
    "\n",
    "            dict_sub[i] = {'start': start_time, 'end': end_time, 'text': text.strip()}\n",
    "            i += 1\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    return dict_sub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new clean subtitles files in output_cleaning folder\n",
    "for file,name in zip(file_with_path,folder):\n",
    "    dict_sub = get_dict_vtt_clean(file)\n",
    "    m.create_vtt_file(dict_sub,f\"{output_cleaning}/{name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a sentence file\n",
    "\n",
    "From the **clean subtitles** files (**output_cleaning** folder), we generate `.txt` files. These new files contains one sentence per line. They are put in a new folder : **output_sent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_with_path = m.lister_fichiers_with_path(output_cleaning)\n",
    "folder = m.lister_fichiers(output_cleaning)\n",
    "output_sent = \"test_sent_file_matignon1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check existing output_sent folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le dossier 'test_sent_file_matignon1' existe déjà.\n"
     ]
    }
   ],
   "source": [
    "message = m.verifier_ou_creer_dossier(output_sent)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sentences files (.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r' [A-Z]\\.')\n",
    "for file,name in zip(file_with_path,folder):\n",
    "    dict_sub = get_dict_vtt_clean(file)\n",
    "    new_text = \"\"\n",
    "    for k,v in dict_sub.items():\n",
    "        for kk,vv in v.items():\n",
    "            if kk == \"text\":\n",
    "                new_text = new_text + vv.strip() + \" \"\n",
    "    phrases = m.segmenter_texte_en_phrases(new_text)\n",
    "    with open(f\"{output_sent}/{name}\", 'w', encoding='utf-8') as f:\n",
    "        for index, element in enumerate(phrases):\n",
    "            f.write(element.strip())\n",
    "            if index < len(phrases) - 1:\n",
    "                f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get an oversegmented subtitle's file (.vtt)\n",
    "\n",
    "From the clean data, we segment our clean subtitle's file (.vtt) following the strong punctuation. We allow the over segmentation, because we can concatenate the subunite after this processing operation. The new subtitle's files are in the **output_seg** folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_with_path = m.lister_fichiers_with_path(output_cleaning)\n",
    "folder = m.lister_fichiers(output_cleaning)\n",
    "output_seg = \"test_new_seg_matignon1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check existing output_seg folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le dossier 'test_new_seg_matignon1' existe déjà.\n"
     ]
    }
   ],
   "source": [
    "message = m.verifier_ou_creer_dossier(output_seg)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File's (over)segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAITEMENT test_cleaning_matignon1/1AjRdJ5d_Ww.vtt ---- 1AjRdJ5d_Ww.vtt\n",
      "resultat : ['Bien.', 'Bonjour,']\n",
      "1.7690000000000001\n",
      "00:00:08.390\n",
      "start time (<class 'str'>) : 00:00:07.759, end time (<class 'str'>) : 00:00:08.390, text : Bien.\n",
      "00:00:09.400\n",
      "start time (<class 'str'>) : 00:00:08.390, end time (<class 'str'>) : 00:00:09.400, text : Bonjour,\n",
      "resultat : ['ne veut pas dire satisfaisante.', \"L'épidémie ne recule plus.\"]\n",
      "3.835000000000008\n",
      "00:01:55.247\n",
      "start time (<class 'str'>) : 00:01:53.198, end time (<class 'str'>) : 00:01:55.247, text : ne veut pas dire satisfaisante.\n",
      "00:01:56.966\n",
      "start time (<class 'str'>) : 00:01:55.247, end time (<class 'str'>) : 00:01:56.966, text : L'épidémie ne recule plus.\n",
      "resultat : ['Le travail[SUSPENSION].', 'Je veux insister sur le fait']\n",
      "2.835000000000008\n",
      "00:02:20.847\n",
      "start time (<class 'str'>) : 00:02:19.594, end time (<class 'str'>) : 00:02:20.847, text : Le travail[SUSPENSION].\n",
      "00:02:22.373\n",
      "start time (<class 'str'>) : 00:02:20.847, end time (<class 'str'>) : 00:02:22.373, text : Je veux insister sur le fait\n",
      "resultat : [\"Ce n'est qu'un début.\", \"Et d'ici à la fin du mandat,\"]\n",
      "3.0330000000000155\n",
      "00:09:26.164\n",
      "start time (<class 'str'>) : 00:09:24.891, end time (<class 'str'>) : 00:09:26.164, text : Ce n'est qu'un début.\n",
      "00:09:27.862\n",
      "start time (<class 'str'>) : 00:09:26.164, end time (<class 'str'>) : 00:09:27.862, text : Et d'ici à la fin du mandat,\n",
      "resultat : ['pour répondre à vos questions.', 'Madame Tassin.']\n",
      "2.9009999999999536\n",
      "00:10:16.264\n",
      "start time (<class 'str'>) : 00:10:14.330, end time (<class 'str'>) : 00:10:16.264, text : pour répondre à vos questions.\n",
      "00:10:17.166\n",
      "start time (<class 'str'>) : 00:10:16.264, end time (<class 'str'>) : 00:10:17.166, text : Madame Tassin.\n",
      "resultat : ['sur comment cela va se dérouler en France ?', 'Merci.']\n",
      "2.461999999999989\n",
      "00:10:54.414\n",
      "start time (<class 'str'>) : 00:10:52.297, end time (<class 'str'>) : 00:10:54.414, text : sur comment cela va se dérouler en France ?\n",
      "00:10:54.709\n",
      "start time (<class 'str'>) : 00:10:54.414, end time (<class 'str'>) : 00:10:54.709, text : Merci.\n",
      "resultat : ['Je précise[SUSPENSION].', 'Pardon.']\n",
      "1.461999999999989\n",
      "00:13:16.183\n",
      "start time (<class 'str'>) : 00:13:15.099, end time (<class 'str'>) : 00:13:16.183, text : Je précise[SUSPENSION].\n",
      "00:13:16.513\n",
      "start time (<class 'str'>) : 00:13:16.183, end time (<class 'str'>) : 00:13:16.513, text : Pardon.\n",
      "resultat : ['Oui.', \"Elizabeth Pineau de l'agence Reuters.\"]\n",
      "1.5609999999999218\n",
      "00:13:51.478\n",
      "start time (<class 'str'>) : 00:13:51.330, end time (<class 'str'>) : 00:13:51.478, text : Oui.\n",
      "00:13:52.853\n",
      "start time (<class 'str'>) : 00:13:51.478, end time (<class 'str'>) : 00:13:52.853, text : Elizabeth Pineau de l'agence Reuters.\n",
      "resultat : ['Merci beaucoup.', \"C'était justement l'objet de ma question.\"]\n",
      "3.824999999999932\n",
      "00:13:54.072\n",
      "start time (<class 'str'>) : 00:13:53.066, end time (<class 'str'>) : 00:13:54.072, text : Merci beaucoup.\n",
      "00:13:56.823\n",
      "start time (<class 'str'>) : 00:13:54.072, end time (<class 'str'>) : 00:13:56.823, text : C'était justement l'objet de ma question.\n",
      "resultat : ['Vous, par exemple, quel sera votre programme ?', 'Merci.']\n",
      "3.867999999999938\n",
      "00:19:03.885\n",
      "start time (<class 'str'>) : 00:19:00.528, end time (<class 'str'>) : 00:19:03.885, text : Vous, par exemple, quel sera votre programme ?\n",
      "00:19:04.322\n",
      "start time (<class 'str'>) : 00:19:03.885, end time (<class 'str'>) : 00:19:04.322, text : Merci.\n",
      "resultat : ['sur une grande radio nationale ?', 'Merci.']\n",
      "4.363000000000056\n",
      "00:22:53.744\n",
      "start time (<class 'str'>) : 00:22:50.165, end time (<class 'str'>) : 00:22:53.744, text : sur une grande radio nationale ?\n",
      "00:22:54.415\n",
      "start time (<class 'str'>) : 00:22:53.744, end time (<class 'str'>) : 00:22:54.415, text : Merci.\n"
     ]
    }
   ],
   "source": [
    "for file,name in zip(file_with_path,folder):\n",
    "    print(f\"TRAITEMENT {file} ---- {name}\")\n",
    "    dict_sub = get_dict_vtt_clean(file)\n",
    "    new_dict = {}\n",
    "    mm = 0\n",
    "    pattern = r'([.!?]+)[^)]' # good solution\n",
    "    sous_unite = []\n",
    "    for k, v in dict_sub.items():\n",
    "        for kk, vv in v.items():\n",
    "            if kk == \"text\":\n",
    "                # Replace the point between two capital letters with '#'\n",
    "                modified_text = re.sub(r'(?<=[A-Z])\\.(?=[A-Z])', '#', vv) #attention à les supprimer pour remettre les points à la place à la fin\n",
    "                # Use re.split() to split the text based on the pattern\n",
    "                sentences = re.split(pattern, modified_text)\n",
    "                # Combine pairs of adjacent list elements (sentence + punctuation)\n",
    "                result = [sentences[i] + sentences[i + 1] if i < len(sentences) - 1 else sentences[i] for i in range(0, len(sentences), 2)]\n",
    "                # Remove empty strings from the result\n",
    "                result = [sentence.strip() for sentence in result if sentence.strip()]\n",
    "                if len(result) == 1:\n",
    "                    if mm not in new_dict:\n",
    "                        new_dict[mm]=v\n",
    "                        mm = mm +1\n",
    "                else:\n",
    "                    if result:\n",
    "                        print(f\"resultat : {result}\")\n",
    "                        start_time_str = v[\"start\"]\n",
    "                        end_time_str = v[\"end\"]\n",
    "                        # start_time = m.conv_str_to_time(start_time_str)\n",
    "                        # end_time = m.conv_str_to_time(end_time_str)\n",
    "                        nb_of_carach = len(v[\"text\"])\n",
    "                        duration = m.time_to_seconds(end_time_str) - m.time_to_seconds(start_time_str)\n",
    "                        duration_sec = duration\n",
    "                        print(duration)\n",
    "                        sec_par_letter = duration_sec / nb_of_carach\n",
    "                        for match in result:\n",
    "                            len_match = len(match)\n",
    "                            duration_match = len_match*sec_par_letter\n",
    "                            if mm not in new_dict:\n",
    "                                #print(end_time)\n",
    "                                end_time = m.ajouter_secondes(start_time_str,duration_match)\n",
    "                                print(end_time)\n",
    "                                print(f\"start time ({type(start_time_str)}) : {start_time_str}, end time ({type(end_time)}) : {end_time}, text : {match}\")\n",
    "                                new_dict[mm]={'start':start_time_str,\"end\":end_time,'text':match}\n",
    "                                start_time_str = end_time\n",
    "                                mm = mm +1\n",
    "                    else:\n",
    "                        continue\n",
    "    m.create_vtt_file(new_dict,f\"{output_seg}/{name}\")\n",
    "\n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate the subunite from the subtitles in sentences\n",
    "\n",
    "We now have two main folders : \n",
    "1. One containing `.txt` files, with a sentence per line\n",
    "2. On containing `.vtt` files, with the subtitles segmented over strong punctuation\n",
    "\n",
    "We can now use the sentence files to concatenate the subunit. The final subtitles are saved in **output_folder** folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Matignon-LSF one file\n",
    "pre_seg_file = output_seg\n",
    "sentence_file = output_sent\n",
    "resultat = m.lister_fichiers_with_path(pre_seg_file)\n",
    "path_sentence = m.lister_fichiers_with_path(sentence_file)\n",
    "resultat_output = m.lister_fichiers(pre_seg_file)\n",
    "sentences_only = m.lister_fichiers(sentence_file)\n",
    "output_folder = \"test_sent_seg_matignon1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check existing output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le dossier 'test_sent_seg_matignon1/' existe déjà.\n"
     ]
    }
   ],
   "source": [
    "message = m.verifier_ou_creer_dossier(output_folder)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new subtitles files - sent based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAITEMENT : 1AjRdJ5d_Ww.vtt && 1AjRdJ5d_Ww.vtt\n",
      "TRAITEMENT : test_new_seg_matignon1/1AjRdJ5d_Ww.vtt && test_sent_file_matignon1/1AjRdJ5d_Ww.vtt\n"
     ]
    }
   ],
   "source": [
    "# Creation of the new subtitle's files.\n",
    "\n",
    "for sub,sub_name,phr,phr_name in zip(resultat,resultat_output,path_sentence,sentences_only):\n",
    "    print(f\"TRAITEMENT : {sub_name} && {phr_name}\")\n",
    "    print(f\"TRAITEMENT : {sub} && {phr}\")\n",
    "    dict_sub = m.get_dict_vtt(sub)\n",
    "    sentences = m.get_sentences(phr)\n",
    "    keys = list(dict_sub.keys())\n",
    "    i = 0\n",
    "    j = 0\n",
    "    mm = 0\n",
    "    new_dict = {}\n",
    "    content = \"\"\n",
    "    while i < len(keys) and j < len(sentences):\n",
    "        #print(mm)\n",
    "        key = keys[i]\n",
    "        value = dict_sub[key]\n",
    "        unite = value['text'].strip()\n",
    "        sent = sentences[j].strip()\n",
    "        #print(f\"'{unite} --> '{sent}'\")\n",
    "        # Je regarde si l'unité est dans la phrase. Si elle est dans la phrase, j'ajoute l'information start, et tant que les unités sont dans la même phrase je concatène pour ajouter l'information du texte\n",
    "        # I check if the unit is in the sentence. If it is, I add the start information, and as long as the units are in the same sentence, I concatenate to add the text information.\n",
    "        if unite in sent:\n",
    "            #print(f\"'{unite} --> '{sent}'\")\n",
    "            if mm not in new_dict:\n",
    "                new_dict[mm] = {\"start\": value['start']}\n",
    "            if 'text' not in new_dict[mm]:\n",
    "                new_dict[mm][\"text\"] = unite.strip()\n",
    "            else:\n",
    "                new_dict[mm]['text'] += f\" {unite.strip()}\" # adding space here, problem to check\n",
    "            i = i +1\n",
    "\n",
    "            if i == len(keys):\n",
    "                new_dict[mm]['end'] = value['end']\n",
    "        else:\n",
    "            old_key = keys[i - 1]\n",
    "            if mm in new_dict and 'end' not in new_dict[mm]:\n",
    "                new_dict[mm]['end'] = dict_sub[old_key]['end']\n",
    "                #print(new_dict)\n",
    "                mm += 1\n",
    "            j = j +1\n",
    "    m.create_vtt_file(new_dict,f\"{output_folder}/{sub_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An other segmentation if necessary\n",
    "<div style=\"border: 1px solid red; padding: 10px; background-color: #333;\">\n",
    "<strong>⚠️ Warning :</strong> if there is a lot of error due to the segmentation, it is possible to do this step one again. Please check your result before doing it again. If you need to, change the following \"rawcells\" in python code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "file_with_path = m.lister_fichiers_with_path(output_folder)\n",
    "folder = m.lister_fichiers(output_folder)\n",
    "output_seg=output_folder"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "for file,name in zip(file_with_path,folder):\n",
    "    print(f\"TRAITEMENT {file} ---- {name}\")\n",
    "    dict_sub = get_dict_vtt_clean(file)\n",
    "    new_dict = {}\n",
    "    mm = 0\n",
    "    pattern = r'([.!?]+)[^)]' # bonne solution\n",
    "    sous_unite = []\n",
    "    for k, v in dict_sub.items():\n",
    "        for kk, vv in v.items():\n",
    "            if kk == \"text\":\n",
    "                # Replace the point between two capital letters with '#'\n",
    "                modified_text = re.sub(r'(?<=[A-Z])\\.(?=[A-Z])', '#', vv)\n",
    "                # Use re.split() to split the text based on the pattern\n",
    "                sentences = re.split(pattern, modified_text)\n",
    "                # Combine pairs of adjacent list elements (sentence + punctuation)\n",
    "                result = [sentences[i] + sentences[i + 1] if i < len(sentences) - 1 else sentences[i] for i in range(0, len(sentences), 2)]\n",
    "                # Remove empty strings from the result\n",
    "                result = [sentence.strip() for sentence in result if sentence.strip()]\n",
    "                if len(result) == 1:\n",
    "                    if mm not in new_dict:\n",
    "                        new_dict[mm]=v\n",
    "                        mm = mm +1\n",
    "                else:\n",
    "                    if result:\n",
    "                        print(f\"resultat : {result}\")\n",
    "                        start_time_str = v[\"start\"]\n",
    "                        end_time_str = v[\"end\"]\n",
    "                        # start_time = m.conv_str_to_time(start_time_str)\n",
    "                        # end_time = m.conv_str_to_time(end_time_str)\n",
    "                        nb_of_carach = len(v[\"text\"])\n",
    "                        duration = m.time_to_seconds(end_time_str) - m.time_to_seconds(start_time_str)\n",
    "                        duration_sec = duration\n",
    "                        print(duration)\n",
    "                        sec_par_letter = duration_sec / nb_of_carach\n",
    "                        for match in result:\n",
    "                            len_match = len(match)\n",
    "                            duration_match = len_match*sec_par_letter\n",
    "                            if mm not in new_dict:\n",
    "                                #print(end_time)\n",
    "                                end_time = m.ajouter_secondes(start_time_str,duration_match)\n",
    "                                print(end_time)\n",
    "                                print(f\"start time ({type(start_time_str)}) : {start_time_str}, end time ({type(end_time)}) : {end_time}, text : {match}\")\n",
    "                                new_dict[mm]={'start':start_time_str,\"end\":end_time,'text':match}\n",
    "                                start_time_str = end_time\n",
    "                                mm = mm +1\n",
    "                    else:\n",
    "                        continue\n",
    "    m.create_vtt_file(new_dict,f\"{output_seg}/{name}\")\n",
    "\n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Verification\n",
    "\n",
    "We can check if the **output_folder** matches the **output_sent** folder. In other words, we check if the subtitle units match the corresponding lines in the sentence file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_sent_seg_matignon1/1AjRdJ5d_Ww.vtt']\n"
     ]
    }
   ],
   "source": [
    "output_folder = m.lister_fichiers_with_path(\"test_sent_seg_matignon1\") # change folder\n",
    "output_folder = sorted(output_folder)\n",
    "print(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_sent_file_matignon1/1AjRdJ5d_Ww.vtt']\n"
     ]
    }
   ],
   "source": [
    "path_sentence = m.lister_fichiers_with_path(output_sent) \n",
    "path_sentence = sorted(path_sentence)\n",
    "print(path_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supprimer_derniere_chaine_vide(liste):\n",
    "    if liste and isinstance(liste[-1], str) and not liste[-1]:\n",
    "        del liste[-1]\n",
    "    return liste\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres de fichiers contenant une erreur : 0\n"
     ]
    }
   ],
   "source": [
    "nb_file = 0\n",
    "name_file = []\n",
    "good_file = []\n",
    "for sub_file, sent_file in zip(output_folder,path_sentence):\n",
    "    sub_dict = m.get_dict_vtt(sub_file)\n",
    "    sentence = m.get_sentences(sent_file)\n",
    "    new_sent = []\n",
    "    for k,v in sub_dict.items():\n",
    "        for kk,vv in v.items():\n",
    "            if kk == \"text\":\n",
    "                if \"(... )\" in vv:\n",
    "                    vv = vv.replace(\"(... )\",\"(...)\")\n",
    "                new_sent.append(vv)\n",
    "    new_sent = supprimer_derniere_chaine_vide(new_sent)\n",
    "    differences = m.comparer_listes(new_sent, sentence)\n",
    "    if differences:\n",
    "        print(f\"TRAITEMENT : {sub_file} && {sent_file}\")\n",
    "        for position, element_a, element_b in differences:\n",
    "            nb_file = nb_file +1\n",
    "            print(f\"Différence à la position {position}: {element_a} vs {element_b}\")\n",
    "            name_file.append(sub_file)\n",
    "            break\n",
    "    else:\n",
    "        good_file.append(sub_file)\n",
    "\n",
    "print(f\"Nombres de fichiers contenant une erreur : {nb_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse cleaning\n",
    "\n",
    "We used some special token to replace some challenging one from the subtitles. We do a reverse cleaning in order to have the original subtitle's token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le dossier 'reverse_cleaning_test1' existe déjà.\n"
     ]
    }
   ],
   "source": [
    "reverse_cleaning_file = \"reverse_cleaning_test1\"\n",
    "message = m.verifier_ou_creer_dossier(reverse_cleaning_file)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitutions = [\n",
    "#     (\"(\", \"&#40;\"),\n",
    "#     (\")\", \"&#41;\"),\n",
    "#     (\"?\", \"&#63;\"),\n",
    "#     (\"!\", \"&#33;\"),\n",
    "#     (\".\", \"&#46;\"),\n",
    "#     (\"...\", \"&#8230;\")\n",
    "# ]\n",
    "\n",
    "# def remplacer_entites_html(chaine, substitutions):\n",
    "#     for new, old in substitutions:\n",
    "#         chaine = chaine.replace(old, new)\n",
    "#     return chaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_with_path = m.lister_fichiers_with_path(\"test_sent_seg_matignon1\")\n",
    "name = m.lister_fichiers(\"test_sent_seg_matignon1\")\n",
    "\n",
    "\n",
    "for file,name_file in zip(files_with_path,name):\n",
    "    dict_sub = reverse_cleaning(file)\n",
    "    m.create_vtt_file(dict_sub,f\"{reverse_cleaning_file}/{name_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lexenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
